
### Submission by Sachin Dangayach (sachin.dangayach@gmail.com)

# Objective

## Replace the embeddings of this session's code with GloVe embeddings and explain the difference between -

***[Session code](https://colab.research.google.com/drive/19pMTnQlp6YItdmIuj5V3s9X7VR1W0sF9?usp=sharing)*** and ***[Your code](https://colab.research.google.com/drive/1P_G1h-M1-Z87tzur8D1dmCk9Kp2ZfBZA)*** .

## Solution:

| Git Link     | Colab Link     |
| :------------- | :------------- |
| Code with pretrained GloVe embeddings (English to French translation)      | https://colab.research.google.com/drive/1P_G1h-M1-Z87tzur8D1dmCk9Kp2ZfBZA |
| Without pretrained embeddings (English to French translation)      | https://colab.research.google.com/drive/1JR0E0aVHCd-sqYU_aPQjAprUYWGrDi37?usp=sharing |

### Concepts -

- **An Attention based Seq2Seq Model**

![Attention based Seq2Seq Model](https://github.com/SachinDangayach/END2.0/blob/main/Session10/images/i_1.PNG)

An attention based Seq2Seq model has three component -

  - Encoder : It takes the input and generates the context vector based on the input sequence.

  - Decoder : It takes the input from the Encoder network and influenced by the attention head to determine which part of the input from encoder along with the last output should be given importance for next output

  - Attention : It helps to focus on what part of input should be focused for the next output

- **Teacher Forcing**

  While training an encoder decoder model, for the output generation from decoder, the next output is influenced by the previous output. As initially, the network is not trained well and the probability of having incorrect output is high, it can impact the next subsequent outputs and make the entire learning process difficult. The hack to improve the learning process in a way is to overwrite the output generated by decoder sometime ( based on the probability decided while training ) by the actual output and thus force network to learn the correct output prediction.

### Steps to change the code to use pretrained GloVe embeddings and also change code to translate from English to French

  - ** Data Preparations ** : Here as we want to use GloVe embeddings and as we have the GloVe embeddings from English to French, I have done few changes in the exiting code.

    - Make Lang 1 as English and Lang 2 as French so that we can have the pair with Lang 1 to Lang 2 translation as English to French. To implement this I have called the function prepareData data with reverse = False. Also, I have changed the filter function used to reduce the dataset size as now in the pair first sentence is from English on which we are applying the filter.

    - ** Lang Class Modification **

      - Download the GloVe embeddings. I have downloaded the embeddings from torchtext.vocab. The shape of GloVe vectors used in code in (40000,100) meaning it contains embeddings for 40K words with vector size of 100.

      ![Attention based Seq2Seq Model](https://github.com/SachinDangayach/END2.0/blob/main/Session10/images/i_2.PNG)

      - Modify Lang class by adding a new variable named embedding. For, SOS_token and EOS_token and the tensor of Zeros.

      - Write a helper function get_vector to return the GloVe embedding vector for any word added into the Language vocabulary so that we have finally glove embeddings for all the words in the vocabulary. Also in case, there is no GloVe embedding found for any given word, add a zero tensor for shape (1, 100) to the embedding tensor. Refer the code below.

      ![GloVe embeddings](https://github.com/SachinDangayach/END2.0/blob/main/Session10/images/i_3.PNG)

    - Follow the sequence of already existing code to get the dataset, encoder, decoder and attention class.

    - As we have embedding vector size of 100, we change the size of input layer to 100 from 256.

    - We instantiate the model classes and load the pretrained embeddings to the encoder model embeddings layer as mentioned in the code below -

    ![Load pretrained embeddings](https://github.com/SachinDangayach/END2.0/blob/main/Session10/images/i_4.PNG)

    - Train the model. Below is the snapsot of training log-

    ![Train Log](https://github.com/SachinDangayach/END2.0/blob/main/Session10/images/i_5.PNG)

    - Evaluation

      - I have used the text used for earlier evaluation converted to English using the google translate. The output of models seems to be good as can we verified in the snapshot below.

    ![Evaluationss](https://github.com/SachinDangayach/END2.0/blob/main/Session10/images/i_6.PNG)


### Steps to change the code to translate from English to French

- As we need to compare the performance of model between model using GloVe embedding and without word embeddings, I have converted the existing code to use English as Lang 1 and retrained the model. Also to have fair comparison, I have made the input embedding dimension to be 100 only.

- Change the examples for model evaluation as similar to what we used to evaluate the model with GloVe embeddings.

- Below are training and evaluation snapshot

![Original Model Train Log](https://github.com/SachinDangayach/END2.0/blob/main/Session10/images/i_7.PNG)

![Original Model Evaluation](https://github.com/SachinDangayach/END2.0/blob/main/Session10/images/i_8.PNG)

## Observations

- Below mentioned table summarizes the evaluation results mentioned above for test examples used for both models -

| S. No. | Input in English    | Model 1 with GloVe embeddings    | Model 2 without GloVe embeddings     | Results validated with google translate    |
| :------------- | :------------- |:------------- |:------------- |:------------- |
| 1.      | she is five years younger than me | elle a deux ans plus que moi . <EOS> | elle est ans ans plus . que moi . <EOS> | **Model 1 preforms better** |
| 2.      | she is too small | elle est trop . <EOS> | elle est trop et ur . <EOS> | **Both model results are bad** |
| 3.      | i am not afraid of dying | je n ai pas peur . <EOS> | je ne ai pas peur avec . <EOS>| **Both model results are bad** |
| 4.      | he is a young director full of talent | c est un homme de jeune . . <EOS> | c est un homme de de de . . .| **Model 1 preforms better** |

**Cleary model with GloVe word embeddings performs better**
